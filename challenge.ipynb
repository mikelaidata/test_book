{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tubular Data Science Coding Test\n",
    "In this notebook, we have a dataset of user comments for youtube videos related to animals or pets. We will attempt to identify cat or dog owners based on these comments, find out the topics important to them, and then identify video creators with the most viewers that are cat or dog owners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+--------------------+\n",
      "| creator_name|userid|             comment|\n",
      "+-------------+------+--------------------+\n",
      "| Doug The Pug|  87.0|I shared this to ...|\n",
      "| Doug The Pug|  87.0|  Super cute  üòÄüêïüê∂|\n",
      "|  bulletproof| 530.0|stop saying get e...|\n",
      "|Meu Zool√≥gico| 670.0|Tenho uma jiboia ...|\n",
      "|       ojatro|1031.0|I wanna see what ...|\n",
      "+-------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "\"\"\"\n",
    "schema = StructType([\n",
    "    StructField(\"creator_name\", StringType(), True),\n",
    "    StructField(\"userid\", IntegerType(), True),\n",
    "    StructField(\"comment\", StringType(), True)])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = spark.read.load(\"animals_comments.csv.gz\", format='csv', header = True, inferSchema = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- creator_name: string (nullable = true)\n",
      " |-- userid: double (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the type of \"userid\" to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('userid', df['userid'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- creator_name: string (nullable = true)\n",
      " |-- userid: integer (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+--------------------+\n",
      "| creator_name|userid|             comment|\n",
      "+-------------+------+--------------------+\n",
      "| Doug The Pug|    87|I shared this to ...|\n",
      "| Doug The Pug|    87|  Super cute  üòÄüêïüê∂|\n",
      "|  bulletproof|   530|stop saying get e...|\n",
      "|Meu Zool√≥gico|   670|Tenho uma jiboia ...|\n",
      "|       ojatro|  1031|I wanna see what ...|\n",
      "+-------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 5820035 rows.\n"
     ]
    }
   ],
   "source": [
    "print \"There are a total of {} rows.\".format(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2537174 users.\n"
     ]
    }
   ],
   "source": [
    "print \"There are {} users.\".format(df.select('userid').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4241 creators.\n"
     ]
    }
   ],
   "source": [
    "print \"There are {} creators.\".format(df.select('creator_name').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if there are any NULLs or NANs in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------+\n",
      "|creator_name|userid|comment|\n",
      "+------------+------+-------+\n",
      "|       32050|   565|   1051|\n",
      "+------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------+\n",
      "|creator_name|userid|comment|\n",
      "+------------+------+-------+\n",
      "|           0|     0|      0|\n",
      "+------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are indeed quite a lof of NULLs. For \"creator_name\", I will just fill the missing strings with \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.fillna('unknown', subset=['creator_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------+\n",
      "|creator_name|userid|comment|\n",
      "+------------+------+-------+\n",
      "|           0|   565|   1051|\n",
      "+------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"comment\", on the other hand, we just have to remove all the rows with NULLs since we can not extract any informtion from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clean = df.dropna(subset = ['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------+\n",
      "|creator_name|userid|comment|\n",
      "+------------+------+-------+\n",
      "|           0|     0|      0|\n",
      "+------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.select([count(when(col(c).isNull(), c)).alias(c) for c in df_clean.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, after removing the rows without comments, there are no NULLs in \"userid\", either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan of Attack\n",
    "\n",
    "If a user does not explicitly says she/he has a dog or cat in the comment, there is no way we can tell whether she/he has a dog a cat. Therefore, strictly speaking we can only determine whether a user mentions she/he has a dog or cat.\n",
    "<br>\n",
    "\n",
    "My plan of attack is described as follows. First, I will select (1) a subset of comments in which the users mention \"my dog\", \"my cat\", \"I have a dog\", or \"I have a cat\" (positive examples), and (2) a subset of comments in which the users do not use the words \"my\" and \"have\" (negative examples). The first subset are the comments we are sure the users of which have dogs/cats, and the second subset are the comments we are sure the users of which do not mention that they have dogs/cats. The comments that do not fall into these two groups are the ones I will use a classifier to decide. Some of these comments might mention something like \"I have 2 cats\" (more than one), or \"I have a Corgi\" (use the breed name instead). \n",
    "<br>\n",
    "\n",
    "I will train a classifier using the two selected groups of comments above and then make predictoins for the remaining comments. Because a user might mention the breed names instead of explicitly using the word \"dog\" or \"cat\", I will vectorize all the comments using the [Word2Vec](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.feature.Word2Vec) model, in order to capture similar semantic information between differnt comments (such as \"dog\" v.s \"Corgi\")\n",
    "<br>\n",
    "\n",
    "Lastly, note that what I will model and predict are individual comments instead of individual users. I will not group the comments by the users, given that a user might say totally unrealated things in different comments. As a result, a user will be considered a dog/cat owner as long as one of her/his comments do mention it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Identify Cat or Dog Owners and Users Who Have No Pets\n",
    "\n",
    "Now I will create a new column \"dog_cat\" in the data frame. The rows whose values are True in this column are the first subset (labeled positive examples) I defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cond = (df_clean[\"comment\"].like(\"%my dog%\") | df_clean[\"comment\"].like(\"%I have a dog%\")\\\n",
    "        | df_clean[\"comment\"].like(\"%my cat%\") | df_clean[\"comment\"].like(\"%I have a cat%\"))\n",
    "\n",
    "df_clean = df_clean.withColumn('dog_cat',  cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+-------+\n",
      "|        creator_name|userid|             comment|dog_cat|\n",
      "+--------------------+------+--------------------+-------+\n",
      "|        Doug The Pug|    87|I shared this to ...|  false|\n",
      "|        Doug The Pug|    87|  Super cute  üòÄüêïüê∂|  false|\n",
      "|         bulletproof|   530|stop saying get e...|  false|\n",
      "|       Meu Zool√≥gico|   670|Tenho uma jiboia ...|  false|\n",
      "|              ojatro|  1031|I wanna see what ...|  false|\n",
      "|     Tingle Triggers|  1212|Well shit now Im ...|  false|\n",
      "|Hope For Paws - O...|  1806|when I saw the en...|  false|\n",
      "|Hope For Paws - O...|  2036|Holy crap. That i...|  false|\n",
      "|          Life Story|  2637|Ê≠¶Âô®„ÅØ„ÇØ„Ç®„Çπ„Éà„ÅßË≤∞„Åà„Çã„Çì„Åò„ÇÉ„Å™„ÅÑ„Çì...|  false|\n",
      "|       Brian Barczyk|  2698|Call the teddy Larry|  false|\n",
      "+--------------------+------+--------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create another new column \"no_pet\". The rows whose values are True in this column are the second subset (labeled negative examples) I defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clean = df_clean.withColumn('no_pet', ~df_clean[\"comment\"].like(\"%my%\") & ~df_clean[\"comment\"].like(\"%have%\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+-------+------+\n",
      "|        creator_name|userid|             comment|dog_cat|no_pet|\n",
      "+--------------------+------+--------------------+-------+------+\n",
      "|        Doug The Pug|    87|I shared this to ...|  false| false|\n",
      "|        Doug The Pug|    87|  Super cute  üòÄüêïüê∂|  false|  true|\n",
      "|         bulletproof|   530|stop saying get e...|  false| false|\n",
      "|       Meu Zool√≥gico|   670|Tenho uma jiboia ...|  false|  true|\n",
      "|              ojatro|  1031|I wanna see what ...|  false|  true|\n",
      "|     Tingle Triggers|  1212|Well shit now Im ...|  false|  true|\n",
      "|Hope For Paws - O...|  1806|when I saw the en...|  false|  true|\n",
      "|Hope For Paws - O...|  2036|Holy crap. That i...|  false|  true|\n",
      "|          Life Story|  2637|Ê≠¶Âô®„ÅØ„ÇØ„Ç®„Çπ„Éà„ÅßË≤∞„Åà„Çã„Çì„Åò„ÇÉ„Å™„ÅÑ„Çì...|  false|  true|\n",
      "|       Brian Barczyk|  2698|Call the teddy Larry|  false|  true|\n",
      "+--------------------+------+--------------------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build and Evaluate Classifiers\n",
    "\n",
    "In order to train a model against the comments, I will use [RegexTokenizer](https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.ml.feature.RegexTokenizer) to split each comment into a list of words and then use [Word2Vec](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.feature.Word2Vec) to convert the list to a word vector. What Word2Vec does is to map each word to a unique fixed-size vector and then transform each document into a vector using the average of all words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+-------+------+--------------------+\n",
      "|        creator_name|userid|             comment|dog_cat|no_pet|                text|\n",
      "+--------------------+------+--------------------+-------+------+--------------------+\n",
      "|        Doug The Pug|    87|I shared this to ...|  false| false|[i, shared, this,...|\n",
      "|        Doug The Pug|    87|  Super cute  üòÄüêïüê∂|  false|  true|       [super, cute]|\n",
      "|         bulletproof|   530|stop saying get e...|  false| false|[stop, saying, ge...|\n",
      "|       Meu Zool√≥gico|   670|Tenho uma jiboia ...|  false|  true|[tenho, uma, jibo...|\n",
      "|              ojatro|  1031|I wanna see what ...|  false|  true|[i, wanna, see, w...|\n",
      "|     Tingle Triggers|  1212|Well shit now Im ...|  false|  true|[well, shit, now,...|\n",
      "|Hope For Paws - O...|  1806|when I saw the en...|  false|  true|[when, i, saw, th...|\n",
      "|Hope For Paws - O...|  2036|Holy crap. That i...|  false|  true|[holy, crap, that...|\n",
      "|          Life Story|  2637|Ê≠¶Âô®„ÅØ„ÇØ„Ç®„Çπ„Éà„ÅßË≤∞„Åà„Çã„Çì„Åò„ÇÉ„Å™„ÅÑ„Çì...|  false|  true|                  []|\n",
      "|       Brian Barczyk|  2698|Call the teddy Larry|  false|  true|[call, the, teddy...|\n",
      "+--------------------+------+--------------------+-------+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"comment\", outputCol=\"text\", pattern=\"\\\\W\")\n",
    "df_clean = regexTokenizer.transform(df_clean)\n",
    "df_clean.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import split\n",
    "#df_clean = df_clean.withColumn(\"text\", split(\"comment\", \"\\s+\"))\n",
    "#df_clean.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here I randomly choose 100000 examples from the original dataset for all the following procedures. The full dataset will be processed with databricks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "\n",
    "df_clean.orderBy(rand(seed=0)).createOrReplaceTempView(\"table\")\n",
    "t = spark.sql(\"select * from table limit 100000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will use Word2Vec to create a new column called \"vector\". The default value of ``vectorSize`` is 100, but in order to reduce the computation time, here I only use ``vectorSize`` = 10. Using a higher dimension should lead to a better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=10, minCount=0, inputCol=\"text\", outputCol=\"vector\", seed=0)\n",
    "model = word2Vec.fit(t)\n",
    "t = model.transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+-------+------+--------------------+--------------------+\n",
      "|        creator_name| userid|             comment|dog_cat|no_pet|                text|              vector|\n",
      "+--------------------+-------+--------------------+-------+------+--------------------+--------------------+\n",
      "|     Gohan The Husky| 106460|keep HIM! and cal...|  false|  true|[keep, him, and, ...|[-0.0758979980332...|\n",
      "|          Real Shock|1050336|–Ø –Ω–∞ –≤—Å–µ –ø—Ä–∞–≤–∏–ª—å–Ω...|  false|  true|                  []|          (10,[],[])|\n",
      "|       Brian Barczyk| 636384| HOGWARTS üêçüêçüêçüêçüêç|  false|  true|          [hogwarts]|[-0.0186032876372...|\n",
      "|  Think Like A Horse|1411449|I love how you al...|  false| false|[i, love, how, yo...|[-0.0615040621986...|\n",
      "|            M·∫°nh CFM| 879606|      Hai vai a manh|  false|  true| [hai, vai, a, manh]|[-0.5470096990466...|\n",
      "| Home Aquatics Hobby|1995671|      Imma try this!|  false|  true|   [imma, try, this]|[-0.3004563550154...|\n",
      "|             Sad Cat|2039444|The 2nd dog was t...|  false|  true|[the, 2nd, dog, w...|[-0.1361289360345...|\n",
      "|Hope For Paws - O...| 565118|My eyes on tears ...|  false|  true|[my, eyes, on, te...|[0.26825467466066...|\n",
      "|         FROSTY Life| 892104|Nature protection...|  false|  true|[nature, protecti...|[0.07621853252251...|\n",
      "|            ViralHog|2270703|      Damn she thick|  false|  true|  [damn, she, thick]|[-0.0493705272674...|\n",
      "+--------------------+-------+--------------------+-------+------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will filter the confirmed positive examples, which are the rows with ``dog_cat = True`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 615 confirmed positive examples.\n"
     ]
    }
   ],
   "source": [
    "t_label_pos = t.filter(t.dog_cat == True)\n",
    "print \"There are {} confirmed positive examples.\".format(t_label_pos.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ones we will predict are those rows with ``dog_cat = False`` and ``no_pet = False``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14385 examples that we are going to predict.\n"
     ]
    }
   ],
   "source": [
    "t_unknown = t.filter((t.dog_cat == False) & (t.no_pet == False))\n",
    "print \"There are {} examples that we are going to predict.\".format(t_unknown.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confirmed negative examples are the rows with ``no_pet = True``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 85000 confirmed negative examples.\n"
     ]
    }
   ],
   "source": [
    "print \"There are {} confirmed negative examples.\".format(t.filter(t.no_pet == True).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is over 100 times more than the confirmed positive examples... To avoid using a unbalanced dataset for model training, I just randomly sample 1000 rows from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t.filter(t.no_pet == True).orderBy(rand(seed=0)).createOrReplaceTempView(\"table\")\n",
    "t_label_neg = spark.sql(\"select * from table where limit 1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine the positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|dog_cat|              vector|\n",
      "+-------+--------------------+\n",
      "|      1|[-0.0390798664755...|\n",
      "|      1|[-0.1118405458370...|\n",
      "|      1|[-0.0028305472806...|\n",
      "|      1|[0.02547704771553...|\n",
      "|      1|[0.03385367389354...|\n",
      "|      1|[-0.0884501119454...|\n",
      "|      1|[0.03460733519624...|\n",
      "|      1|[0.06628606600376...|\n",
      "|      1|[-0.0940878521806...|\n",
      "|      1|[-0.0592414590840...|\n",
      "+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_label = t_label_pos.union(t_label_neg).select('dog_cat','vector')\n",
    "t_label = t_label.withColumn('dog_cat',  t_label.dog_cat.cast('integer'))\n",
    "t_label.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will split these labeled data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = t_label.randomSplit([0.7, 0.3], seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose a random forest classifier for this binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.classification import LogisticRegression\n",
    "#from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator #,MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can actually use K-fold cross validation to tune the model's hyperparameters. But again, in order to reduce my computation time, I'll skip this procedure. I will just use a random forest with 10 trees and leave all the other parameters at their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyspark.ml.tuning import CrossValidator\\nfrom pyspark.ml.tuning import ParamGridBuilder\\n\\nrf = RandomForestClassifier(labelCol=\"dog_cat\", featuresCol=\"vector\", numTrees=10)\\n\\ngrid = ParamGridBuilder().addGrid(rf.maxDepth, [4, 5, 6]).build()\\n\\nevaluator = BinaryClassificationEvaluator(\\n    labelCol=\"dog_cat\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\\n\\ncv = CrossValidator(estimator=rf, estimatorParamMaps=grid, evaluator=evaluator)\\ncvModel = cv.fit(train)\\npredictions = model.transform(test)\\n\\nprint evaluator.evaluate(cvModel.transform(train))\\nprint evaluator.evaluate(predictions)\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"dog_cat\", featuresCol=\"vector\", numTrees=10)\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(rf.maxDepth, [4, 5, 6]).build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"dog_cat\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "cvModel = cv.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "print evaluator.evaluate(cvModel.transform(train))\n",
    "print evaluator.evaluate(predictions)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the random forest against the training set and predict for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+----------+\n",
      "|dog_cat|              vector|       rawPrediction|         probability|prediction|\n",
      "+-------+--------------------+--------------------+--------------------+----------+\n",
      "|      1|[-0.3882409960031...|[4.34092039636372...|[0.43409203963637...|       1.0|\n",
      "|      1|[-0.2765789677699...|[4.68853457157885...|[0.46885345715788...|       1.0|\n",
      "|      1|[-0.2682929776608...|[9.30002301178203...|[0.93000230117820...|       0.0|\n",
      "|      1|[-0.2387377321720...|[3.78149485443725...|[0.37814948544372...|       1.0|\n",
      "|      1|[-0.2311467430912...|[4.73215266872314...|[0.47321526687231...|       1.0|\n",
      "|      1|[-0.1856541619636...|[4.76012818252756...|[0.47601281825275...|       1.0|\n",
      "|      1|[-0.1793941780924...|[4.61924106815534...|[0.46192410681553...|       1.0|\n",
      "|      1|[-0.1785858562216...|[4.74656255906810...|[0.47465625590681...|       1.0|\n",
      "|      1|[-0.1707417197259...|[2.63424319738626...|[0.26342431973862...|       1.0|\n",
      "|      1|[-0.1702176103275...|[3.79525379775040...|[0.37952537977504...|       1.0|\n",
      "+-------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"dog_cat\", featuresCol=\"vector\", numTrees=10)\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"dog_cat\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "AUC = evaluator.evaluate(predictions)\n",
    "\n",
    "TP = predictions[(predictions[\"dog_cat\"] == 1) & (predictions[\"prediction\"] == 1.0)].count()\n",
    "FP = predictions[(predictions[\"dog_cat\"] == 0) & (predictions[\"prediction\"] == 1.0)].count()\n",
    "TN = predictions[(predictions[\"dog_cat\"] == 0) & (predictions[\"prediction\"] == 0.0)].count()\n",
    "FN = predictions[(predictions[\"dog_cat\"] == 1) & (predictions[\"prediction\"] == 0.0)].count()\n",
    "\n",
    "accuracy = (TP + TN)*1.0 / (TP + FP + TN + FN)\n",
    "precision = TP*1.0 / (TP + FP)\n",
    "recall = TP*1.0 / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 130\n",
      "False Positives: 55\n",
      "True Negatives: 241\n",
      "False Negatives: 41\n",
      "Test Accuracy: 0.79443254818\n",
      "Test Precision: 0.702702702703\n",
      "Test Recall: 0.760233918129\n",
      "Test AUC of ROC: 0.872935435435\n"
     ]
    }
   ],
   "source": [
    "print \"True Positives:\", TP\n",
    "print \"False Positives:\", FP\n",
    "print \"True Negatives:\", TN\n",
    "print \"False Negatives:\", FN\n",
    "print \"Test Accuracy:\", accuracy\n",
    "print \"Test Precision:\", precision\n",
    "print \"Test Recall:\", recall\n",
    "print \"Test AUC of ROC:\", AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+----------+----+\n",
      "|dog_cat|              vector|       rawPrediction|         probability|prediction|pred|\n",
      "+-------+--------------------+--------------------+--------------------+----------+----+\n",
      "|      1|[-0.3882409960031...|[4.34092039636372...|[0.43409203963637...|       1.0|   1|\n",
      "|      1|[-0.2765789677699...|[4.68853457157885...|[0.46885345715788...|       1.0|   0|\n",
      "|      1|[-0.2682929776608...|[9.30002301178203...|[0.93000230117820...|       0.0|   0|\n",
      "|      1|[-0.2387377321720...|[3.78149485443725...|[0.37814948544372...|       1.0|   1|\n",
      "|      1|[-0.2311467430912...|[4.73215266872314...|[0.47321526687231...|       1.0|   0|\n",
      "|      1|[-0.1856541619636...|[4.76012818252756...|[0.47601281825275...|       1.0|   0|\n",
      "|      1|[-0.1793941780924...|[4.61924106815534...|[0.46192410681553...|       1.0|   0|\n",
      "|      1|[-0.1785858562216...|[4.74656255906810...|[0.47465625590681...|       1.0|   0|\n",
      "|      1|[-0.1707417197259...|[2.63424319738626...|[0.26342431973862...|       1.0|   1|\n",
      "|      1|[-0.1702176103275...|[3.79525379775040...|[0.37952537977504...|       1.0|   1|\n",
      "+-------+--------------------+--------------------+--------------------+----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test: how the evaluation metrics change when we tune the threshold of the classifier\n",
    "Maybe we can use a for loop to optimize the threshold. But what defines the best threshold...?   \n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "# get the 2nd element of \"probability\" vector, which is the probability of the example being positive, \n",
    "# and then use a new threshold (other than 0.5) to make predictions\n",
    "thresh = 0.55\n",
    "element = udf(lambda v: int(float(v[1]) > thresh) , IntegerType())\n",
    "tmp = predictions.select(element('probability'))\n",
    "tmp = tmp.withColumnRenamed(\"<lambda>(probability)\", \"pred\")\n",
    "\n",
    "# since there is no common column between these two dataframes (\"predictions\" and \"tmp\"), we add \n",
    "# \"row_index\" so they can be joined\n",
    "predictions = predictions.withColumn('row_index', f.monotonically_increasing_id())\n",
    "tmp = tmp.withColumn('row_index', f.monotonically_increasing_id())\n",
    "\n",
    "predictions = predictions.join(tmp, on=[\"row_index\"]).sort(\"row_index\").drop(\"row_index\")\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 118\n",
      "False Positives: 39\n",
      "True Negatives: 257\n",
      "False Negatives: 53\n",
      "Test Accuracy: 0.802997858672\n",
      "Test Precision: 0.751592356688\n",
      "Test Recall: 0.690058479532\n"
     ]
    }
   ],
   "source": [
    "TP = predictions[(predictions[\"dog_cat\"] == 1) & (predictions[\"pred\"] == 1)].count()\n",
    "FP = predictions[(predictions[\"dog_cat\"] == 0) & (predictions[\"pred\"] == 1)].count()\n",
    "TN = predictions[(predictions[\"dog_cat\"] == 0) & (predictions[\"pred\"] == 0)].count()\n",
    "FN = predictions[(predictions[\"dog_cat\"] == 1) & (predictions[\"pred\"] == 0)].count()\n",
    "\n",
    "accuracy = (TP + TN)*1.0 / (TP + FP + TN + FN)\n",
    "precision = TP*1.0 / (TP + FP)\n",
    "recall = TP*1.0 / (TP + FN)\n",
    "\n",
    "print \"True Positives:\", TP\n",
    "print \"False Positives:\", FP\n",
    "print \"True Negatives:\", TN\n",
    "print \"False Negatives:\", FN\n",
    "print \"Test Accuracy:\", accuracy\n",
    "print \"Test Precision:\", precision\n",
    "print \"Test Recall:\", recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 3: Classify All The Users\n",
    "\n",
    "We can now apply the cat/dog classifiers to all the other users in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14385 examples that we are going to predict.\n"
     ]
    }
   ],
   "source": [
    "print \"There are {} examples that we are going to predict.\".format(t_unknown.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "t_unknown.withColumn('dog_cat', t_unknown.dog_cat.cast('integer'))\n",
    "pred = model.transform(t_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 90460 users.\n"
     ]
    }
   ],
   "source": [
    "n_total = t.select('userid').distinct().count()\n",
    "print \"There are a total of {} users.\".format(n_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7101 users that are confirmed or predicted to be dog/cat owners.\n",
      "About 7.85% of the users are dog/cat owners \n"
     ]
    }
   ],
   "source": [
    "# The number of users with at least one comment that is labeled as positive by us\n",
    "n_pos = t_label_pos.select('userid').distinct().count()\n",
    "\n",
    "# The number of users with at least one comment that is predicted to be positive\n",
    "n_pred = pred.filter(pred.prediction == 1).select('userid').distinct().count()\n",
    "\n",
    "print \"There are {} users that are confirmed or predicted to be dog/cat owners.\".format(n_pos + n_pred)\n",
    "print \"About {:.2f}% of the users are dog/cat owners \".format((n_pos + n_pred)*1.0/n_total*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My idea was to use the test precision/recall to estimate the true number of comments telling us the users are dog/cat owners. But the problem is...**\n",
    "- **The number of comments may not really translate to the number of uers**\n",
    "- **The distribution of our labeled and labeled are definitly not the same**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Insights About Cat And Dog Owners\n",
    "\n",
    "Goal: Find topics important to cat and dog owners.\n",
    "\n",
    "**Get word counts and then show the top-k most frequent words??**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Identify Creators With Cat And Dog Owners In The Audience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Find creators with the most cat and/or dog owners. Find creators with the highest statistically significant percentages of cat and/or dog owners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
